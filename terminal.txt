[2025-05-14 19:06:14,073] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[INFO|2025-05-14 19:06:17] llamafactory.cli:143 >> Initializing 8 distributed tasks at: 127.0.0.1:38267
[2025-05-14 19:06:24,551] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-14 19:06:24,551] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-14 19:06:24,552] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-14 19:06:24,556] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-14 19:06:24,556] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-14 19:06:24,556] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-14 19:06:24,561] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-14 19:06:24,561] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-14 19:06:26,541] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-14 19:06:26,561] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-14 19:06:26,577] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-14 19:06:26,608] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-14 19:06:26,631] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-14 19:06:26,632] [INFO] [comm.py:700:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-05-14 19:06:26,636] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-14 19:06:26,683] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-14 19:06:26,703] [INFO] [comm.py:669:init_distributed] cdb=None
[INFO|2025-05-14 19:06:28] llamafactory.hparams.parser:143 >> Set `ddp_find_unused_parameters` to False in DDP training since LoRA is enabled.
[INFO|2025-05-14 19:06:28] llamafactory.hparams.parser:401 >> Process rank: 0, world size: 8, device: cuda:0, distributed training: True, compute dtype: torch.bfloat16
[INFO|2025-05-14 19:06:28] llamafactory.hparams.parser:401 >> Process rank: 5, world size: 8, device: cuda:5, distributed training: True, compute dtype: torch.bfloat16
[INFO|2025-05-14 19:06:28] llamafactory.hparams.parser:401 >> Process rank: 3, world size: 8, device: cuda:3, distributed training: True, compute dtype: torch.bfloat16
[INFO|2025-05-14 19:06:28] llamafactory.hparams.parser:401 >> Process rank: 1, world size: 8, device: cuda:1, distributed training: True, compute dtype: torch.bfloat16
[INFO|2025-05-14 19:06:28] llamafactory.hparams.parser:401 >> Process rank: 2, world size: 8, device: cuda:2, distributed training: True, compute dtype: torch.bfloat16
[INFO|2025-05-14 19:06:28] llamafactory.hparams.parser:401 >> Process rank: 4, world size: 8, device: cuda:4, distributed training: True, compute dtype: torch.bfloat16
[INFO|2025-05-14 19:06:28] llamafactory.hparams.parser:401 >> Process rank: 7, world size: 8, device: cuda:7, distributed training: True, compute dtype: torch.bfloat16
[INFO|2025-05-14 19:06:29] llamafactory.data.template:143 >> Replace eos token: <|eot_id|>.
[WARNING|2025-05-14 19:06:29] llamafactory.data.template:148 >> New tokens have been added, make sure `resize_vocab` is True.
[INFO|2025-05-14 19:06:29] llamafactory.data.template:143 >> Add <|eom_id|> to stop words.
[WARNING|2025-05-14 19:06:29] llamafactory.data.template:148 >> New tokens have been added, make sure `resize_vocab` is True.
[INFO|2025-05-14 19:06:29] llamafactory.data.loader:143 >> Loading dataset identity.json...
