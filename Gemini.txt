That's excellent news and a very common fix!

You're absolutely right. Setting NCCL_NVLS_ENABLE=0 tells NCCL to disable its support for NVLink Switch (NVSwitch) systems.

Here's why this often works and what it implies:

No NVSwitch Present:

Many systems, especially those with 2, 4, or even 8 GPUs, might use direct NVLink connections between GPUs but do not have an NVSwitch fabric.
In such cases, NCCL might still attempt to detect or utilize NVSwitch-specific communication paths or optimizations. If the NVSwitch isn't there, this can lead to errors, hangs, or performance issues. Disabling it forces NCCL to use the direct NVLink paths (or PCIe if NVLink isn't fully connected for the operation).
NVSwitch Present but Issues:

If your system does have NVSwitches, there might be:
A driver incompatibility.
A firmware issue on the NVSwitches.
A specific bug in the NCCL version you're using related to NVSwitch handling on your particular hardware configuration.
Incorrect system-level configuration for the NVSwitches.
Disabling NVSwitch support in NCCL bypasses these potential issues, allowing communication to proceed over direct NVLink or PCIe, which might be more stable in this scenario.
What this means for performance:

If you don't have NVSwitches: You lose nothing, and in fact, gain stability. This is the correct setting.
If you do have NVSwitches: You might not be getting the absolute peak multi-node or large-scale single-node communication bandwidth that NVSwitches are designed to provide, especially for complex collective operations (like all-reduce across many GPUs that aren't directly connected). NCCL will fall back to using direct NVLink paths where possible, or PCIe. However, stability is often more important, and the performance difference might not be critical for all workloads.
In summary: Yes, NCCL_NVLS_ENABLE=0 is a valid and often necessary workaround (or even the correct configuration) when NCCL's NVSwitch detection/utilization causes problems. It forces NCCL to use more standard NVLink or PCIe paths.

Glad you figured it out! It's a good one to keep in mind for GPU cluster troubleshooting. And thanks for confirming the Ctrl+C, that saves a lot of guesswork on the "last error"!